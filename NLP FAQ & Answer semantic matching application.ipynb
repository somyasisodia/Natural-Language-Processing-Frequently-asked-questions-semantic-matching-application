{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "import logging\n",
    "import json\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk import pos_tag\n",
    "import pysolr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file=open(\"C:/Users/Somya/Anaconda3/Library/bin/train.txt\",\"r\")\n",
    "\n",
    "questions = list()\n",
    "answers = list()\n",
    "questions_bag = list()\n",
    "answers_bag = list()\n",
    "question_answer_bag=list()\n",
    "faq_bag=list()\n",
    "filtered_sentences=list()\n",
    "lemm_bag=list()\n",
    "stem_bag=list()\n",
    "pos_tagged_bag=list()\n",
    "synset_bag=list()\n",
    "hypernym_bag=list()\n",
    "hyponym_bag=list()\n",
    "holonym_bag=list()\n",
    "meronym_bag=list()\n",
    "parsing_bag=list()\n",
    "        \n",
    "for line in file:\n",
    "    if line.strip():\n",
    "        if line.startswith(\"Q:\"):\n",
    "            questions.append(line)\n",
    "            questions_bag.append(word_tokenize(line.strip()))\n",
    "        else:\n",
    "            answers.append(line)\n",
    "            answers_bag.append(word_tokenize(line.strip()))\n",
    "\n",
    "\n",
    "for i in range(len(questions)):\n",
    "    faq_bag.append(questions_bag[i]+answers_bag[i])\n",
    "    question_answer_bag.append(questions[i]+answers[i])\n",
    "    \n",
    "# REMOVING STOP WORDS\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "for i in range(len(questions)):\n",
    "    filtered_sentence = [w for w in faq_bag[i] if not w in stop_words]\n",
    "    filtered_sentences.append(filtered_sentence)\n",
    "    \n",
    "\n",
    "    \n",
    "   \n",
    "for sentence in filtered_sentences:\n",
    "    bag1=list()\n",
    "    bag2=list()\n",
    "    bag3=list()\n",
    "    bag4=list()\n",
    "    for word in sentence:\n",
    "        word=wn.synsets(word)\n",
    "    \n",
    "        for l in word:\n",
    "            for hyponym in l.hyponyms():\n",
    "                for lemma in hyponym.lemmas():\n",
    "                    bag1.append(lemma.name())\n",
    "                    \n",
    "            for hypernym in l.hypernyms():\n",
    "                for lemma in hypernym.lemmas():\n",
    "                    bag2.append(lemma.name())\n",
    "                    \n",
    "            for holonym in l.member_holonyms():\n",
    "                for lemma in holonym.lemmas():\n",
    "                    bag3.append(lemma.name())\n",
    "                    \n",
    "            for meronym in l.member_meronyms():\n",
    "                for lemma in meronym.lemmas():\n",
    "                    bag4.append(lemma.name())\n",
    "                    \n",
    "    hyponym_bag.append(bag1)\n",
    "    hypernym_bag.append(bag2)\n",
    "    holonym_bag.append(bag3)\n",
    "    meronym_bag.append(bag4)\n",
    " \n",
    "    \n",
    "#path_to_jar=\"G:/Somya/Semester_Spring/NLP/Project/stanford-parser-full-2018-02-27/stanford-parser.jar\"\n",
    "#path_to_models_jar='G:/Somya/Semester_Spring/NLP/Project/stanford-parser-full-2018-02-27/stanford-parser-3.9.1-models.jar'\n",
    "#dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "nlp = StanfordCoreNLP(r'G:\\Somya\\Semester_Spring\\NLP\\Project\\stanford-corenlp-full-2017-06-09')\n",
    "for l in question_answer_bag:\n",
    "    bag=list()\n",
    "    for sentence in sent_tokenize(l):\n",
    "        #dep = next(dependency_parser.raw_parse(sentence))\n",
    "        bag.append(nlp.dependency_parse(sentence))\n",
    "    parsing_bag.append(bag)\n",
    "\n",
    "#LEMMATIZE WORDS\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for l in filtered_sentences:\n",
    "    lemm=list()\n",
    "    for word in l:\n",
    "        lemm.append(lemmatizer.lemmatize(word))\n",
    "    lemm_bag.append(lemm)\n",
    "\n",
    "\n",
    "#STEM WORDS\n",
    "stemmer = PorterStemmer()\n",
    "for l in filtered_sentences:\n",
    "    stem=list()\n",
    "    for word in l:\n",
    "        stem.append(stemmer.stem(word))\n",
    "    stem_bag.append(stem)\n",
    "\n",
    "\n",
    "#POS TAGGING\n",
    "for l in faq_bag:\n",
    "    tokens_pos= pos_tag(l)\n",
    "    pos_tagged_bag.append(tokens_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('C:/Users/Somya/NlpProjectFeatureOutputs/WordsWithoutStopWords.txt', \"w\") as text_file:\n",
    "    for l in filtered_sentences:\n",
    "            text_file.write('%s\\n' %l)\n",
    "            \n",
    "with open('C:/Users/Somya/NlpProjectFeatureOutputs/Lemmas.txt', \"w\") as text_file:\n",
    "    for l in lemm_bag:\n",
    "            text_file.write('%s\\n' %l)\n",
    "            \n",
    "with open('C:/Users/Somya/NlpProjectFeatureOutputs/Stems.txt', \"w\") as text_file:\n",
    "    for l in stem_bag:\n",
    "            text_file.write('%s\\n' %l)\n",
    "            \n",
    "with open('C:/Users/Somya/NlpProjectFeatureOutputs/POS_Tag.txt', \"w\") as text_file:\n",
    "    for l in pos_tagged_bag:\n",
    "            text_file.write('%s\\n' %l)\n",
    "            \n",
    "with open('C:/Users/Somya/NlpProjectFeatureOutputs/ParsedSentences.txt', \"w\") as text_file:\n",
    "    for l in parsing_bag:\n",
    "            text_file.write('%s\\n' %l)\n",
    "            \n",
    "with open('C:/Users/Somya/NlpProjectFeatureOutputs/Holonyms.txt', \"w\") as text_file:\n",
    "    for l in holonym_bag:\n",
    "            text_file.write('%s\\n' %l)\n",
    "            \n",
    "with open('C:/Users/Somya/NlpProjectFeatureOutputs/Hypernyms.txt', \"w\") as text_file:\n",
    "    for l in hypernym_bag:\n",
    "            text_file.write('%s\\n' %l)\n",
    "            \n",
    "with open('C:/Users/Somya/NlpProjectFeatureOutputs/Meronyms.txt', \"w\") as text_file:\n",
    "    for l in meronym_bag:\n",
    "            text_file.write('%s\\n' %l)\n",
    "            \n",
    "with open('C:/Users/Somya/NlpProjectFeatureOutputs/Hyponyms.txt', \"w\") as text_file:\n",
    "    for l in hyponym_bag:\n",
    "            text_file.write('%s\\n' %l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features=list()\n",
    "for i in range(len(questions)):\n",
    "    features.append(filtered_sentences[i] + lemm_bag[i] + stem_bag[i] + pos_tagged_bag[i] + parsing_bag[i] + holonym_bag[i] + hypernym_bag[i] + meronym_bag[i] + hyponym_bag[i])\n",
    "    #features.extend(lemm_bag[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Somya\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n",
      "C:\\Users\\Somya\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "enter the FAQ?  EC2 scaled pricing\n",
      "[ 0 51 14  2 40  1 41 47 35 42]\n",
      "Q: What is Amazon Elastic Compute Cloud (Amazon EC2)?\n",
      "\n",
      "Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides resizable compute capacity in the cloud. It is designed to make web-scale computing easier for developers.\n",
      "\n",
      "\n",
      "\n",
      "Q: How does Amazon EC2 Auto Scaling balance capacity?\n",
      "\n",
      "Balancing resources across Availability Zones is a best practice for well-architected applications, as this greatly increases aggregate system availability. Amazon EC2 Auto Scaling automatically balances EC2 instances across zones when you configure multiple zones in your EC2 Auto Scaling group settings. Amazon EC2 Auto Scaling always launches new instances such that they are balanced between zones as evenly as possible across the entire fleet. What’s more, Amazon EC2 Auto Scaling only launches into Availability Zones in which there is available capacity for the requested instance type.\n",
      "\n",
      "\n",
      "\n",
      "Q: Does Amazon EC2 use ECC memory?\n",
      "\n",
      "In our experience, ECC memory is necessary for server infrastructure, and all the hardware underlying Amazon EC2 uses ECC memory.\n",
      "\n",
      "\n",
      "\n",
      "Q: How can I get started with Amazon EC2?\n",
      "\n",
      "To sign up for Amazon EC2, click the “Sign up for This Web Service” button on the Amazon EC2 detail page. You must have an Amazon Web Services account to access this service; if you do not already have one, you will be prompted to create one when you begin the Amazon EC2 sign-up process. After signing up, please refer to the Amazon EC2 documentation, which includes our Getting Started Guide.\n",
      "\n",
      "\n",
      "\n",
      "Q: What is an EC2 Auto Scaling group?\n",
      "\n",
      "An Amazon EC2 Auto Scaling group contains a collection of EC2 instances that share similar characteristics and are treated as a logical grouping for the purposes of fleet management and dynamic scaling. For example, if a single application operates across multiple instances, you might want to increase the number of instances in that group to improve the performance of the application, or decrease the number of instances to reduce costs when demand is low. Amazon EC2 Auto Scaling will automaticallly adjust the number of instances in the group to maintain a fixed number of instances even if a instance becomes unhealthy, or based on criteria that you specify. You can find more information about Amazon EC2 Auto Scaling groups in the Amazon EC2 Auto Scaling User Guide.\n",
      "\n",
      "\n",
      "\n",
      "Q: What can I do with Amazon EC2?\n",
      "\n",
      "Just as Amazon Simple Storage Service (Amazon S3) enables storage in the cloud, Amazon EC2 enables “compute” in the cloud. Amazon EC2’s simple web service interface allows you to obtain and configure capacity with minimal friction. It provides you with complete control of your computing resources and lets you run on Amazon’s proven computing environment. Amazon EC2 reduces the time required to obtain and boot new server instances to minutes, allowing you to quickly scale capacity, both up and down, as your computing requirements change. Amazon EC2 changes the economics of computing by allowing you to pay only for capacity that you actually use.\n",
      "\n",
      "\n",
      "\n",
      "Q: What happens to my Amazon EC2 instances if I delete my EC2 Auto Scaling Group?\n",
      "\n",
      "If you have an EC2 Auto Scaling group with running instances and you choose to delete the Amazon EC2 Auto Scaling group, the instances will be terminated and the EC2 Auto Scaling group will be deleted.\n",
      "\n",
      "\n",
      "\n",
      "Q: Can I launch different types of EC2 instances in same EC2 Auto Scaling group?\n",
      "\n",
      "EC2 Auto Scaling groups optimize for the case when all your instance types are the same. You can use the AttachInstances API to attach instances of different types to an Auto Scaling group, and you can also update your launch configuration so that any new instances in the group will be launched with a different instance type. However, this will not affect any of the existing instances.\n",
      "\n",
      "\n",
      "\n",
      "Q: What is Amazon EC2 Auto Scaling?\n",
      "\n",
      "Amazon EC2 Auto Scaling is a fully managed service designed to launch or terminate Amazon EC2 instances automatically to help ensure you have the correct number of Amazon EC2 instances available to handle the load for your application. Amazon EC2 Auto Scaling helps you maintain application availability through fleet management for EC2 instances, which detects and replaces unhealthy instances, and by scaling your Amazon EC2 capacity up or down automatically according to conditions you define. You can use Amazon EC2 Auto Scaling to automatically increase the number of Amazon EC2 instances during demand spikes to maintain performance and decrease capacity during lulls to reduce costs.\n",
      "\n",
      "\n",
      "\n",
      "Q: How do I know when EC2 Auto Scaling is launching or terminating the EC2 instances in an EC2 Auto Scaling group?\n",
      "\n",
      "When you use Amazon EC2 Auto Scaling to scale your applications automatically, it is useful to know when EC2 Auto Scaling is launching or terminating the EC2 instances in your EC2 Auto Scaling group. Amazon SNS coordinates and manages the delivery or sending of notifications to subscribing clients or endpoints. You can configure EC2 Auto Scaling to send an SNS notification whenever your EC2 Auto Scaling group scales. Amazon SNS can deliver notifications as HTTP or HTTPS POST, email (SMTP, either plain-text or in JSON format), or as a message posted to an Amazon SQS queue. For example, if you configure your EC2 Auto Scaling group to use the autoscaling: EC2_INSTANCE_TERMINATE notification type, and your EC2 Auto Scaling group terminates an instance, it sends an email notification. This email contains the details of the terminated instance, such as the instance ID and the reason that the instance was terminated.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Task 2 Computation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(question_answer_bag)\n",
    "X_train_counts.shape\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "target=list(range(1, 68))\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, target)\n",
    "print(clf)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()), ('clf', MultinomialNB()),])\n",
    "\n",
    "text_clf = text_clf.fit(questions, target)\n",
    "\n",
    "import numpy as np\n",
    "test_list=list()\n",
    "user_input = input(\"enter the FAQ?  \") \n",
    "question=user_input\n",
    "test_list.append(question)\n",
    "predicted = text_clf.predict_proba(test_list)\n",
    "np.mean(predicted == target)\n",
    "x=np.argsort(predicted)[0][-10:]\n",
    "print(x)\n",
    "\n",
    "\n",
    "for result in x:\n",
    "    print(questions[result]+'\\n'+answers[result]+'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Task 4 computation\n",
    "solr = pysolr.Solr('http://localhost:8983/solr/nlpTask4', timeout=10)\n",
    "id=1\n",
    "for l in features:\n",
    "    name='document '+str(id)\n",
    "    doc={'id':id, 'name': name, 'text': l}\n",
    "    solr.add([doc])\n",
    "    id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the FAQ?  How do I know if EC2 is running in more than one regions\n",
      "document 32\n",
      "document 6\n",
      "document 21\n",
      "document 10\n",
      "document 25\n",
      "document 13\n",
      "document 17\n",
      "document 33\n",
      "document 8\n",
      "document 28\n",
      "document 32\n",
      "Q: Is Amazon EC2 running in more than one region?\n",
      "\n",
      "Yes. Please refer to Regional Products and Services for more details of our product and service availability by region.\n",
      "\n",
      "\n",
      "\n",
      "document 6\n",
      "Q: How do I run systems in the Amazon EC2 environment?\n",
      "\n",
      "Once you have set up your account and select or create your AMIs, you are ready to boot your instance. You can start your AMI on any number of On-Demand instances by using the RunInstances API call. You simply need to indicate how many instances you wish to launch. If you wish to run more than 20 On-Demand instances, complete the Amazon EC2 instance request form. If Amazon EC2 is able to fulfill your request, RunInstances will return success, and we will start launching your instances. You can check on the status of your instances using the DescribeInstances API call. You can also programmatically terminate any number of your instances using the TerminateInstances API call. If you have a running instance using an Amazon EBS boot partition, you can also use the StopInstances API call to release the compute resources but preserve the data on the boot partition. You can use the StartInstances API when you are ready to restart the associated instance with the Amazon EBS boot partition. In addition, you have the option to use Spot Instances to reduce your computing costs when you have flexibility in when your applications can run. Read more about Spot Instances for a more detailed explanation on how Spot Instances work. If you prefer, you can also perform all these actions from the AWS Management Console or through the command line using our command line tools, which have been implemented with this web service API.\n",
      "\n",
      "\n",
      "\n",
      "document 21\n",
      "Q: If I have two instances in different regions, how will I be charged for data transfer?\n",
      "\n",
      "Each instance is charged for its data in and data out at Internet Data Transfer rates. Therefore, if data is transferred between these two instances, it is charged at Internet Data Transfer Out for the first instance and at Internet Data Transfer In for the second instance.\n",
      "\n",
      "\n",
      "\n",
      "document 10\n",
      "Q: How do I access my systems?\n",
      "\n",
      "The RunInstances call that initiates execution of your application stack will return a set of DNS names, one for each system that is being booted. This name can be used to access the system exactly as you would if it were in your own data center. You own that machine while your operating system stack is executing on it.\n",
      "\n",
      "\n",
      "\n",
      "document 25\n",
      "Q: How do I select the right instance type?\n",
      "\n",
      "Amazon EC2 instances are grouped into 5 families: General Purpose, Compute Optimized, Memory Optimized, Storage Optimized and Accelerated Computing instances. General Purpose Instances have memory to CPU ratios suitable for most general purpose applications and come with fixed performance (M5, M4) or burstable performance (T2); Compute Optimized instances (C5, C4) have proportionally more CPU resources than memory (RAM) and are well suited for scale out compute-intensive applications and High Performance Computing (HPC) workloads; Memory Optimized Instances (X1e, X1, R4) offer larger memory sizes for memory-intensive applications, including database and memory caching applications; Accelerating Computing instances (P3, P2, G3, F1) take advantage of the parallel processing capabilities of NVIDIA Tesla GPUs for high performance computing and machine/deep learning; GPU Graphics instances (G3) offer high-performance 3D graphics capabilities for applications using OpenGL and DirectX; F1 instances deliver Xilinx FPGA-based reconfigurable computing; Storage Optimized Instances (H1, I3, D2) that provide very high, low latency, I/O capacity using SSD-based local instance storage for I/O-intensive applications, with D2 or H1, the dense-storage and HDD-storage instances, provide local high storage density and sequential I/O performance for data warehousing, Hadoop and other data-intensive applications. When choosing instance types, you should consider the characteristics of your application with regards to resource utilization (i.e. CPU, Memory, Storage) and select the optimal instance family and instance size.\n",
      "\n",
      "\n",
      "\n",
      "document 13\n",
      "Q: How quickly can I scale my capacity both up and down?\n",
      "\n",
      "Amazon EC2 provides a truly elastic computing environment. Amazon EC2 enables you to increase or decrease capacity within minutes, not hours or days. You can commission one, hundreds or even thousands of server instances simultaneously. When you need more instances, you simply call RunInstances, and Amazon EC2 will typically set up your new instances in a matter of minutes. Of course, because this is all controlled with web service APIs, your application can automatically scale itself up and down depending on its needs.\n",
      "\n",
      "\n",
      "\n",
      "document 17\n",
      "Q: How will I be charged and billed for my use of Amazon EC2?\n",
      "\n",
      "You pay only for what you use. Displayed pricing is an hourly rate but depending on which instances you choose, you pay by the hour or second (minimum of 60 seconds) for each instance type. Partial instance-hours consumed are billed based on instance usage. Data transferred between AWS services in different regions will be charged as Internet Data Transfer on both sides of the transfer. Usage for other Amazon Web Services is billed separately from Amazon EC2. For EC2 pricing information, please visit the pricing section on the EC2 detail page.\n",
      "\n",
      "\n",
      "\n",
      "document 33\n",
      "Q: How can I make sure that I am in the same Availability Zone as another developer?\n",
      "\n",
      "We do not currently support the ability to coordinate launches into the same Availability Zone across AWS developer accounts. One Availability Zone name (for example, us-east-1a) in two AWS customer accounts may relate to different physical Availability Zones.\n",
      "\n",
      "\n",
      "\n",
      "document 8\n",
      "Q: How quickly will systems be running?\n",
      "\n",
      "It typically takes less than 10 minutes from the issue of the RunInstances call to the point where all requested instances begin their boot sequences. This time depends on a number of factors including: the size of your AMI, the number of instances you are launching, and how recently you have launched that AMI. Images launched for the first time may take slightly longer to boot.\n",
      "\n",
      "\n",
      "\n",
      "document 28\n",
      "Q: How do I prevent other people from viewing my systems?\n",
      "\n",
      "You have complete control over the visibility of your systems. The Amazon EC2 security systems allow you to place your running instances into arbitrary groups of your choice. Using the web services interface, you can then specify which groups may communicate with which other groups, and also which IP subnets on the Internet may talk to which groups. This allows you to control access to your instances in our highly dynamic environment. Of course, you should also secure your instance as you would any other server.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Task 4 results\n",
    "user_input = input(\"enter the FAQ?  \") \n",
    "results = solr.search(user_input)\n",
    "\n",
    "for result in results:\n",
    "    print(result['name'])\n",
    "\n",
    "for result in results:\n",
    "    print(result['name'])\n",
    "    print(questions[int(result['id'])-1]+'\\n'+answers[int(result['id'])-1]+'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the FAQ?  How can I get informed about AWS security?\n",
      "[29  2 28 46 36 44  1 12 48 54]\n",
      "\n",
      "\n",
      "Q: Where can I find more information about security on AWS?\n",
      "\n",
      "For more information on security on AWS please refer to our Amazon Web Services: Overview of Security Processes white paper and to our Amazon EC2 running Windows Security Guide.\n",
      "\n",
      "\n",
      "\n",
      "Q: How can I get started with Amazon EC2?\n",
      "\n",
      "To sign up for Amazon EC2, click the “Sign up for This Web Service” button on the Amazon EC2 detail page. You must have an Amazon Web Services account to access this service; if you do not already have one, you will be prompted to create one when you begin the Amazon EC2 sign-up process. After signing up, please refer to the Amazon EC2 documentation, which includes our Getting Started Guide.\n",
      "\n",
      "\n",
      "\n",
      "Q: Can I get a history of all EC2 API calls made on my account for security analysis and operational troubleshooting purposes? \n",
      "\n",
      "Yes. To receive a history of all EC2 API calls (including VPC and EBS) made on your account, you simply turn on CloudTrail in the AWS Management Console.  For more information, visit the CloudTrail home page. \n",
      "\n",
      "\n",
      "\n",
      "Q: Can EC2 Auto Scaling groups span multiple AWS regions?\n",
      "\n",
      "EC2 Auto Scaling groups are regional constructs. They can span Availability Zones, but not AWS regions.\n",
      "\n",
      "\n",
      "\n",
      "Q: When should I use Amazon EC2 Auto Scaling vs. AWS Auto Scaling?\n",
      "\n",
      "You should use AWS Auto Scaling if you want more guidance on defining your application scaling plan, or if you want to scale multiple resources beyond EC2, such as Amazon DynamoDB tables and indexes, or Amazon ECS tasks. At this time, to use AWS Auto Scaling, you must create your applications via AWS CloudFormation or AWS Elastic Beanstalk. AWS Auto Scaling helps you manage all your scaling policies in one place for your applications making tuning easy and intuitive. You should use Amazon EC2 Auto Scaling if you only need to scale Amazon EC2 Auto Scaling Groups, or just want to maintain the health of your EC2 fleet.\n",
      "\n",
      "\n",
      "\n",
      "Q: How many instances can an EC2 Auto Scaling group have?\n",
      "\n",
      "You can have as many instances in your EC2 Auto Scaling group as your EC2 quota allows.\n",
      "\n",
      "\n",
      "\n",
      "Q: What can I do with Amazon EC2?\n",
      "\n",
      "Just as Amazon Simple Storage Service (Amazon S3) enables storage in the cloud, Amazon EC2 enables “compute” in the cloud. Amazon EC2’s simple web service interface allows you to obtain and configure capacity with minimal friction. It provides you with complete control of your computing resources and lets you run on Amazon’s proven computing environment. Amazon EC2 reduces the time required to obtain and boot new server instances to minutes, allowing you to quickly scale capacity, both up and down, as your computing requirements change. Amazon EC2 changes the economics of computing by allowing you to pay only for capacity that you actually use.\n",
      "\n",
      "\n",
      "\n",
      "Q: How quickly can I scale my capacity both up and down?\n",
      "\n",
      "Amazon EC2 provides a truly elastic computing environment. Amazon EC2 enables you to increase or decrease capacity within minutes, not hours or days. You can commission one, hundreds or even thousands of server instances simultaneously. When you need more instances, you simply call RunInstances, and Amazon EC2 will typically set up your new instances in a matter of minutes. Of course, because this is all controlled with web service APIs, your application can automatically scale itself up and down depending on its needs.\n",
      "\n",
      "\n",
      "\n",
      "Q: How can I implement changes across multiple instances in an EC2 Auto Scaling group?\n",
      "\n",
      "You can use AWS CodeDeploy or CloudFormation to orchestrate code changes to multiple instances in your EC2 Auto Scaling group.\n",
      "\n",
      "\n",
      "\n",
      "Q: Can I customize a health check?\n",
      "\n",
      "Yes, there is an API called SetInstanceHealth that allows you to change an instance's state to UNHEALTHY, which will then result in a termination and replacement.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Somya\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "#Task 2 results\n",
    "\n",
    "import numpy as np\n",
    "test_list=list()\n",
    "user_input = input(\"enter the FAQ?  \") \n",
    "question=user_input\n",
    "test_list.append(question)\n",
    "predicted = text_clf.predict_proba(test_list)\n",
    "np.mean(predicted == target)\n",
    "x=np.argsort(predicted)[0][-10:]\n",
    "x=x[::-1]\n",
    "print(x)\n",
    "print()\n",
    "print()\n",
    "for result in x:\n",
    "    print(questions[result]+'\\n'+answers[result]+'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
